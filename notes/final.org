* Technical Objectives
  The primary function of the depth estimation algorithm is to take in a predicted focal length, and a pair of stereogram images and to produce an RGBD (Red, Green, Blue, and Depth for each pixel) image.

  The focal length estimator should produce some confidence interval of focal lengths. This focal length is assumed to be roughly the same for both cameras. We also expect to have 2 matching input stereograms, taken from cameras a reasonable distance apart, rotated to focus on a single focus target. These stereograms will most likely be somewhat noisy. There may also be slight variations in rotation, position, or scale between the two images (as they will most likely be scans of physical stereograms). There may also be some distortion due to inconsistencies in the focal length of the lense. This distortion is assumed to be negligible.

  All of these different sources of error must be taken into account by the depth finding algorithm, and it must still be able to produce an accurate RGBD image. In later stages, this RGBD image will be used to find the distance between points in the image. We require the RGBD image to be accurate enough such that the measured distance is within one foot of the real distance.

* Research
  As our goal is to determine depth from a stereogram image, it is useful to first consider how humans percieve depth as a stereogram camera setup is modeled after the human eyes. One easy observation, is that objects appear smaller as they move further away. This is helpful, as if we know the size of an object we can easily determine it's distance using only one eye. However, in some circumstances where the sizes of objects are unknown this is not possible. In these cases, being able to see the same scene from 2 different angles at once can help to resolve the depth of unknown objects. In our depth finding algorithm, we do not know the exact size of many of the objects in our image, so it is necessary to use the information from both images in order to determine the depth of objects in our scene.

  In order to talk more concretely about how a stereogram camera setup mirrors the human eyes, we need the correct mathematical tools. Homogeneous coordinates provide an extremely useful way to build upon the concepts of linear algebra to describe more types of transformations. For this first explanation of homogeneous coordinates, we will focus on 2D, but most of the concepts presented here can be easily extended to 3D as well.

  In traditional cartesian coordinates, a point in 2D is described by a vector with 2 coordinates, such as: <x, y>. In homogeneous coordinates, this same point can be represented by any vector of the form <xZ, yZ, Z>, where Z is a non-zero real number. This new coordinate, Z, acts as a normalization factor: scaling our vector by any non-zero scaling factor yields the same point. To convert back to cartesian coordinates, we only need to divide by the scaling factor.

  What happens when our normalization factor is zero? Intuitively, when we take a point of the form <x, y, 0> and we divide by the scaling factor, we end up with our x and y coordinates at positive or negative infinity. Therefore a vector of the form <x, y, 0> can be seen to represent a point infinitely far away in the direction <x, y> or <-x, -y>. We will call this concept a point at infinity. If we extend this concept to higher dimensions, we can also have lines or planes at infinity, following a similar format.

  Lines are described using the same format <a, b, c>. In this cae, -a/b is the slope, and -c/b is the y-intercept. To determine if some point is on a line, we use the dot product. If p \cdot l = 0, the point p is on the line l. From this we can see, that the vector <a, b, 1> is normal to the line <a, b, c>. The intersection of 2 lines, \alpha and \beta, is \alpha \cross \beta. If the lines are parallel, this intersection will be a point at infinity in the same direction as the lines.

  How do these coordinates help us to model the camera? 
