#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage[parfill]{parskip}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{helvet}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \renewcommand{\familydefault}{\sfdefault}
#+LATEX_HEADER: \setstretch{1.2}
#+LATEX_CLASS_OPTIONS: [12pt]

* Technical Objectives
** Inputs
*** stereograms [0/2] [0%]                                           :ignore:
    \\

    The primary input is the pair of input images, forming a stereogram.
    As these are scans of historical photos, these are expected to be quite noisy.
    In our case, our target historical photo is an image depicting a gravesite with a few pyramids, shown in figure [[img:stereo]].
    The position of the photo centers is also expected to vary between the two images.
    The images in this case are not cropped, so there will be additional noise around the edges.
    These images might have some degree of additional distortion, which we will not know ahead of time.
    
    #+caption: Our target historical stereogram, featuring left and right frames taken simultaneously by 2 cameras, available from [[[loc]]].
    #+name: img:stereo
    #+attr_latex: :height 200px
    [[file:../data/stereo.png]]

*** focal length :ignore:
    \\
    From the focal length estimator, we expect to receive a rough estimate of the focal length.
    This estimate is expected to have some error, so further refinement might be required to achieve a useful depth estimate.
*** additional ground truth data :ignore:
    \\
    The user will also need to specify some ground truth data for the image.
    This data will provide a pair of points in one of the images, as well as a required scale.
    In the case of our target historical photos, this will likely be the length of the pyramid's base.
    Ideally, this will be used to establish the scale of the output depth map.
*** what we won't have :ignore:
    \\
    We don't expect to have accurate camera intrinsics.
    For instance, parameters such as distortion will need to be estimated in case they are not negligible.
    Additionally, we won't have extrinsic parameters for our cameras.
    In other words, we won't know the relative rotation and translation of the right camera from the left.
    We may need to come up with some method of estimating this.

** Output
*** Real-world coordinates :ignore:
    \\
    The primary output of the depth estimation algorithm will be a dense mapping from image coordinates to 3D world coordinates.
    This will likely be relative to one of the camera's reference frames, but we should be able to transform these coordinates to be relative to a landmark in the image, such as the base of a pyramid.

** Acceptance Criteria 
   \\
   Our acceptance criteria will be based on experimental accuracy from a test data set, within some reasonable distance to the camera.
*** Accuracy within 30 cm (~ one foot) for any visible object in the image :ignore:
    We expect the accuracy of depth information to be within 30 cm (approximately one foot) of reality for any object visible in the image, provided that the furthest object is as close as the last visible pyramid in the image.
*** Expected accuracy based on test data set :ignore:
    \\
    Since we don't have ground-truth data for the historical image, we will need to base this estimate on accuracy for a test data set.
    For the test data set, we can estimate the expected percentage of error for depth estimates provided by this algorithm by comparing to ground truth test data.
    We will then assume this expected accuracy is probably accurate to our target historical stereogram.

* Related Work
  Our problem is a variant of structure from motion [[[structure from motion]]].
  We have multiple frames, between which some objects have moved relative to the camera.
  What makes our problem unique is that we don't have the intrinsic or extrinsic parameters of the camera, and we only have 2 images to work with.

** Camera Calibration
   A few solutions already exist that attempt to find camera parameters based on the structure of an image.

*** fSpy :ignore:
    \\
    fSpy is a tool for finding camera focal length [[[ fspy ]]].
**** I/O :ignore:
     It takes in a user-inputted set of pairs of parallel lines in the image, which should be perpendicular.
     Given enough of these pairs, it is able to determine the focal length.
**** why we can't use it :ignore:
     However, fSpy relies on the scene having enough good perpendicular lines to find the focal length.
     While this is typical of a cityscape, this isn't exactly the case for our target historical photo.
     Additionally, this method doesn't account for distortion, so we would need some way of un-distorting our photo before considering this method.
*** Camera calibration technique from Traffic Analysis From Video (Jakub Sochor) :ignore:
    \\
**** I/O :ignore:
     The camera calibration system in [[[sochor]]] is built for video frames taken by traffic cameras.
     Similar to [[[fspy]]] it uses parallel lines to solve for camera parameters.
     However, it uses motion of cars between frames as one set of parallel lines.
**** why we can't use it                                             :ignore:
     If our cameras had the same rotation and the only motion between our left and right frames was translation, we might be able to use the same method to extract lines.
     However, our camera setup likely includes some rotation between the camera frames, so the motion of objects would no longer be parallel.
     This method also relies on undistorted frames.

** Computer Vision Libraries
   \\
   Many tools and libraries exist to perform the different parts of the structure from motion pipeline.
   However, most pre-made solutions are made to solve the problem with slightly different constraints (e.g. many frames, little noise, negligible distortion).
   As this is the case, we will be using general purpose computer vision libraries which provide the tools we need to setup our own pipeline.

*** OpenMVG :ignore:
    \\
    One solid choice of library is OpenMVG [[[openmvg]]].
**** features :ignore:
     OpenMVG provides tools for multiple view geometry calculations, including methods for structure from motion and PnP (perspective-n-points is a related problem for finding camera pose relative to a camera of known dimensions).
**** issues :ignore:
     The only issue is that OpenMVG does not provide Python bindings, which means we would need to write our own wrapper in order to use it for this project.
*** OpenCV :ignore:
    \\
    OpenCV [[[opencv]]] is a mature general purpose computer vision library.
**** features :ignore:
     While OpenCV does not provide many features specifically related to structure from motion, it does provide many of the tools we need to build most of the parts of our pipeline.
     OpenCV also has Python bindings, allowing us to avoid wrapping / distributing native binaries ourselves.
**** issues :ignore:
     The only downside is that OpenCV doesn't provide specific methods for structure from motion, such as bundle adjustment, so we will need to compose these ourselves from other more basic computer-vision building blocks.
*** additional libraries                                             :ignore:
    \\
    In addition to a computer vision library, we will also use a combination of a few math libraries.
    Numpy [[[numpy]]] provides tools for linear algebra and other mathematical operations.
    SciPy [[[scipy]]] provides a least-squares optimizer which will be useful for implementing some methods that OpenCV does not provide.

* Modeling The Problem
  Now that we have a basic understanding of the tools we are working with, we can start analyzing the problem.
** Pinhole Camera Model
   \\
   We will start by defining a good model for the camera.
*** pinhole camera model :ignore:
    [[[camera model notes]]] describes a simplified model of the camera, known as the pinhole camera model.
    This model is made up of a few important parts.

    #+CAPTION: Diagram of a pinhole camera from [[[camera model notes]]].
    [[file:../data/pinhole.png]]

**** pinhole :ignore:
     \\
     In a pinhole camera, the light from our scene is directed through an /aperture/ and exposed on the other side.
     Optimally, this aperture would be a single point, but in a physical camera this would be impossible.
     In a normal camera, this would be the focal point of the lens, where all the incoming light beams intersect.
     We will use this as the origin of the camera's reference coordinate system.

**** aperture :ignore:
     \\
     The plane where our image will be projected is known as the /image plane/.
     We will call the 2D coordinates mapping the intersection of incoming light beams and the image plane /image space/.
     When we are dealing with a digital image, the units of these coordinates is often measured in pixels.

     The /focal length/ of the camera is the distance from the aperture to this image plane.
     It is useful for the focal length to share the units of the image space coordinates, so we will use pixels.

**** optical axis :ignore:
     \\
     The line normal to the aperture passing through the origin of the camera is the /optical axis/.
     The intersection of the optical axis and the aperture is called the /principal point/.
     Another way to see the principal point is the projection of the origin in image space.

*** pinhole camera model approximates real cameras :ignore:
    \\
    As aforementioned, creating a useful pinhole camera is difficult, as we would like the aperture to be a single point, which is impossible with a physical camera.
    In order to make re-focusing the light on a single point possible, most cameras use lenses instead.
**** distortion [0/1] [0%]                                           :ignore:
     These lenses don't necessarily have fixed focal length throughout, possibly due to errors in manufacturing.
     These variations in focal length cause distortion.
     The most common form of distortion is radial distortion, where the focal length of varies as we move away from the center of the lens.
     Figure [[img:distortion]] shows several examples of radial distortion.
     If the distortion is significant, we will need to account for it before we can get accurate measurements.

     #+CAPTION: Examples of the 2 types of radial distortion, caused by variations in focal length of the lens from [[[camera model notes]]].
     #+NAME: img:distortion
     [[file:../data/distortion.png]]

*** getting from some 3D coordinate system to image space            :ignore:
    \\
    Now that we have a reasonable model of the camera, it is important to understand how points in 3D world coordinates are projected to 2D image space.
    This transformation can be split into 2 main steps each defined by a set of parameters.

**** extrinsic properties / pose :ignore:
     The extrinsic parameters of the camera encode the pose information, including the rotation and translation of the camera in world coordinates.
     This especially important if you have multiple cameras, like we do in our stereogram setup.
     For instance, we can use the left camera's reference frame as our world coordinates, making the pose of the right camera relative to the first camera.
**** intrinsic properties / projection :ignore:
     The intrinsic parameters describe the projection from 3D coordinates in our camera's reference system to image space.
     These parameters include the center of our image and the focal length.

** The Math of Projection
   \\
   Now that we have a basic understanding of the camera model, we need a way to represent these transformations using linear operations.

*** homogeneous coordinates :ignore:
    [[[homogeneous coords notes]]] describes a new coordinate system that we can use to describe the type of transformation we need: homogeneous coordinates.
     We will focus on the 2D case for the purpose of example, but the ideas represented here can easily be extended to 3D.
**** basics in 2D [0/2] [0%]                                         :ignore:
     \\
     A point in Cartesian coordinates can be represented as a vector $[x \ y]$, the distance along the coordinate axes from the origin.
     The same point can be written in homogeneous coordinates as a vector $[xZ \ yZ \ Z]$, where $Z$ is a non-zero real number.
     Z acts as a normalization factor: scaling our entire vector by any non-zero scaling factor will yield the same point, as shown in figure [[eq:norm]].

#+CAPTION: The normalization factor makes points represented in homogeneous coordinates invariant to scale.
#+NAME: eq:norm
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    \begin{bmatrix}
      xZ \\
      yZ \\
      Z \\
    \end{bmatrix} =
    Z
    \begin{bmatrix}
      x \\
      y \\
      1 \\
    \end{bmatrix} \propto
    \begin{bmatrix}
      x \\
      y \\
      1 \\
    \end{bmatrix}
    \]
  \end{center}
#+end_figure

***** points at infinity :ignore:
      \\
      If our normalization factor is zero, our point in homogeneous coordinates no longer maps back to a finite point in Cartesian coordinates.
      A vector of the form $[x \ y \ 0]$ represents a point in the direction $[x \ y]$ infinitely far from the origin.
      In higher dimensions, a normalization factor at 0 yields lines and planes at infinity.
***** lines :ignore:
      \\
      Lines are described using the same format as points.
      Given a line described by vector $\lambda = [a \ b \ c]$, and a point described by vector $p = [xZ \ yZ \ Z]$, if $\lambda \cdot p = 0$ the point p is on line \lambda.
      Figure [[eq:line]] shows how we can find the equation of a line written in homogeneous coordinates.

#+CAPTION: The equation of a line from its homogeneous coordinates representation.
#+NAME: eq:line
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
  \[
  \begin{bmatrix}
    xZ \\
    yZ \\
    Z
  \end{bmatrix} \cdot
  \begin{bmatrix}
  a \\
  b \\
  c \\
  \end{bmatrix} = 0
  \]
  \[axZ + byZ + cZ = 0 \]
  \[ y = -(a/b) x - c/b \]
  \end{center}
#+end_figure

      \\
      The slope of lines of the form $[a \ b \ c]$ is $-a/b$, and the y-intercept is at $-c/b$.
      Lines are also invariant to scaling operations, so $\lambda \propto s \lambda$.

****** intersection of lines                                         :ignore:
      The intersection of 2 lines is the cross product.
      If our lines are parallel, this intersection will be a point at infinity in the direction of the lines, as shown in figure [[eq:para_isect]].

#+CAPTION: The intersection of parallel lines.
#+NAME: eq:para_isect
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    \begin{bmatrix}
      a \\
      b \\
      c
    \end{bmatrix} \times
    \begin{bmatrix}
      a \\
      b \\
      d \\
    \end{bmatrix} =
    \begin{bmatrix}
      b*d - b*c \\
      a*c - a*d \\
      a*b - a*b
    \end{bmatrix} \propto
    \begin{bmatrix}
      b \\
      -a \\
      0
    \end{bmatrix}
    \]

  \end{center}
#+end_figure

***** transformations [0/1] [0%]                                     :ignore:
      \\
      Linear transformations in homogeneous coordinates can be modeled as matrices, just like with Cartesian coordinates.
      As homogeneous coordinates add a normalization factor, these matrices have additional degrees of freedom and can represent more types of transformations.
      The most general form of transformation allowed by homogeneous transformation matrices is projection.
      
      In figure [[eq:projection]], $A$ is a 2x2 matrix.
      This matrix can be seen as an affine transformation in Cartesian coordinates, potentially including scaling, rotation, or skew.
      $t$ can be seen as a translation vector, a fixed value that will be added to each coordinate after the transformation represented by A is applied.
      $s$ is a normalization factor.
      Our vectors will be scaled down by this factor after the other operations have been applied.
      $b$ is a skew vector.
      It allows us to break parallelism, lines that are parallel before our transformation no longer need to be parallel after.
      Breaking parallelism allows us to project images with non-orthographic perspectives.

#+CAPTION: Breakdown of a projection matrix into a few key components.
#+NAME: eq:projection
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    P =
    \begin{bmatrix}
      A_{11} & A_{12} & t_{1} \\
      A_{21} & A_{22} & t_{2} \\
      b_{1} & b_{2} & s
    \end{bmatrix} =
    \begin{bmatrix}
      A & t \\
      b & s
    \end{bmatrix}
    \]
  \end{center}
#+end_figure

*** camera intrinsic matrix                                          :ignore:
    \\
    With homogeneous coordinates and projection matrices we can now map from 3D coordinates to the camera's image space using a linear transformation.
    The most basic building block of the projection matrix in the pinhole camera model is the camera intrinsic matrix, $K$.
    $K$ maps from 3D directions in our camera's reference space in Cartesian coordinates, to 2D homogeneous coordinates in image space.
    Note that $K$ will not capture any information about the position of our camera in world coordinates, it assumes that the vectors it transforms are from the camera's origin to a point in 3D space.
    
    The illustration in figure [[fig:projection]] shows how points in 3D are projected to image space.
    A few similar triangles are present in this image, and we can use these to establish a relationship between the parameters of our camera, the point's coordinates in our camera's reference frame, and the coordinates in image space. Figure [[eq:cart_proj]] shows what this relationship would look like in Cartesian coordinates. We need to divide by /z/ to get the intended result, making this relationship non-linear. However homogeneous coordinates allow us to rewrite this relationship as a linear transformation, as shown in [[eq:homo_proj]].

#+CAPTION: Diagram of projection. /O/ is the aperture. /f/ is the focal length of the lens. /P/ is the principal point. /A/ is the point in 3D space, /a/ is the same point on the image plane. $[x \ y \ z]$ specifies the location of /A/ in the camera's reference frame. $[u \ v]$ specifies the location of /a/ in image coordinates.
#+NAME: fig:projection
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{tikzpicture}
    \coordinate [label={below right:$O$}] (O) at (0, 0);
    \coordinate [label={below left:$P$}] (P) at (-4.6, 3);
    \coordinate (P2) at (-11.25, 7.5);
    \coordinate [label={above left:$A$}] (A) at (-9, 9);
    \coordinate [label={above right:$a$}] (a) at (-3.6, 3.6);

    \filldraw[very thick] (O) circle (.05);
    \filldraw[very thick] (P) circle (.05);
    \filldraw[very thick, color=red] (A) circle (.05);
    \filldraw[very thick, color=red] (a) circle (.05);
    \filldraw[very thick, color=blue] (-9, 7.5) circle (.05);
    \filldraw[very thick, color=blue] (-3.6, 3) circle (.05);

    \draw [semithick] (-3, 2) -- (-3, 4) -- (-6, 4) -- (-6, 2) -- (-3, 2);

    \draw [semithick] (0, 0) -- node [below] {$f$} (-4.5, 3) -- node [above] {$z$} (-11.25, 7.5);
    \draw [semithick,color=red] (0, 0) -- (-9, 9);
    \draw [semithick,color=blue] (0, 0) -- (-9, 7.5);

    \draw [semithick] (P) -- node [above] {$u$} (-3.6, 3);
    \draw [semithick] (-3.6, 3) -- node [right] {$v$} (a);

    \draw [semithick] (P2) -- node [below] {$x$} (-9, 7.5);
    \draw [semithick] (-9, 7.5) -- node [left] {$y$} (A);
  \end{tikzpicture}
#+end_figure

#+CAPTION: Solving for our image coordinates given details about our camera, using cartesian coordinates. Here the center of the image is $[c_x \ c_y]$.
#+NAME: eq:cart_proj
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    \begin{bmatrix}
    u - c_x \\
    v - c_y
    \end{bmatrix} / f =
    \begin{bmatrix}
    x \\
    y
    \end{bmatrix} / z
    \]
    \[
    \begin{bmatrix}
      u \\
      v
    \end{bmatrix} =
    \begin{bmatrix}
      f * x / z + p_x \\
      f * y / z + p_y
    \end{bmatrix}
    \]
  \end{center}
#+end_figure

#+CAPTION: Converting our projection equation to homogeneous coordinates and defining the camera intrinsic matrix K.
#+NAME: eq:homo_proj
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    \begin{bmatrix}
    u \\
    v \\
    1
    \end{bmatrix} \propto
    \begin{bmatrix}
      uz \\
      vz \\
      z \\
    \end{bmatrix} =
    \begin{bmatrix}
      f * x + c_x * z \\
      f * y + c_y * z\\
      z
    \end{bmatrix} =
    \begin{bmatrix}
      f & 0 & c_x \\
      0 & f & c_y \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y \\
      z
    \end{bmatrix}
    \]
    \[
    K =
    \begin{bmatrix}
      f & 0 & c_x \\
      0 & f & c_y \\
      0 & 0 & 1
    \end{bmatrix} \
    a =
    \begin{bmatrix}
      u \\
      v \\
      1
    \end{bmatrix} \
    A =
    \begin{bmatrix}
      x \\
      y \\
      z
    \end{bmatrix}
    \]
    \[
      a \propto KA
    \]
  \end{center}
#+end_figure

    This matrix $K$ has a few key parts.
    $[ c_x \ c_y ]$ represent the location of the principal point.
    $f_x$ and $f_y$ are the focal length of the image along the coordinate axes.
    If $f_x = f_y$, the image has square pixels.
    This is a useful assumption that will simplify the problem of solving for $K$.

**** as a mapping from directions to points in image space :ignore:
     \\
     As K maps from 3D to 2D coordinates, there has to be some loss of information in the process.
     As K is a transformation on homographic coordinates, the scale of the input does not affect the output.
     Therefore, all points in the same direction from the camera's origin are mapped to the same point in image space.
     K is a bijective mapping between 3D directions and 2D points in image space.

**** vanishing points :ignore:
     \\
     In 3D, the intersection of 2 parallel lines will be a point at infinity in the same direction as those lines.
     Projecting those lines to image space, we will find that they are no longer parallel (provided they are visible in the image).
     The intersection of these 2 parallel lines in image space is known as a vanishing point.
     By inverting K, we can map this vanishing point to a direction in 3D space in the same direction as our lines.
     This is useful as it allows us to find the angle of a pair of parallel lines in our image.
     Provided we can find 2 sets of parallel lines that lie on the same plane, we can use this technique to find the normal vector for a plane in our image as well.

*** adding pose information                                          :ignore:
    \\
    The intrinsic projection matrix does not include a mapping from world coordinates to camera coordinates.
    This transformation might include translation, so a matrix including this camera pose information would need to map from 3D homogeneous world coordinates to 2D homogeneous image coordinates.

    Typically, this pose information would be represented as a rotation matrix and translation vector for the camera's reference system relative to world coordinates.
    This means we need to apply the inverse of these operations to map the points from world coordinates to camera coordinates: $R_c = R_{p}^{-1}. \ T_c = -R_c T_p$,
    where $R_c, T_c$ is the camera's pose information, and $R_p, T_p$ is the pose of points in the camera's reference system relative to the points in world coordinates.

    If our camera is not rotated or translated relative to the camera coordinate system, we can write the final projection matrix as $P = K [I \ 0]$
    We can incorporate the pose of our points, $R_p, T_p$, as $P = K [ R_p T_p ]$.

** Determining Depth
*** depth with a single camera :ignore:
    \\
    Looking at the model we have so far, we can see that as points move away from the camera's origin along the Z axis, their projections move towards the principal point in 3D space.
    This has the affect of shrinking objects as they move further from the camera, proportional to their distance from the camera.
    Using this information, if we know the length of an object parallel to the camera plane, as well as the focal length, we can determine the distance of the object to the camera.
    However in cases where we don't have the necessary information -- such as the object's measurements -- we won't be able to determine the depth with just the information from a single camera.
**** PnP                                                             :ignore:
     \\
     If we don't know an objects rotation relative to the camera, it might be difficult to determine it's length parallel to the camera plane.
     In cases where we know an object's exact dimensions, there are few methods that we can use to determine it's relative rotation.
***** pose from pairs of parallel lines                              :ignore:
      If our object has appropriate structure we can find its rotation by finding vanishing points.
      This is usually only the case for simple objects.
***** pose in the general case                                       :ignore:
      \\
      In the general case, this class of problems is known as Perspective-n-Points or PnP.
      OpenCV provides a number of solutions for this problem, as described in [[[3pnp]]] and [[[epnp]]].
      As input these functions take in a list of 3D points describing features of our object, as well as a list of 2D points specifying the corresponding features in image space.
      In order to generate a unique solution, these functions need at least 4 points.

*** depth with multiple frames                                       :ignore:
**** triangulation :ignore:
     \\
     With 2 or more frames, we can use the extra information provided by the second camera to triangulate points in 3D, provided we know the relative pose of the other camera, and that we can identify the same point in both images.
     The first step of triangulation would be to use the camera intrinsic matrix to find the directions from each camera to our target point.
     Using each camera's pose information, we can draw lines passing through the camera's origin in the directions obtained from the previous step.
     The intersection of these lines should be the location of the point in world coordinates.
     To account for error, instead of finding the intersection of the lines, we can find the optimal point using least squares optimization.

**** global feature matching :ignore:
     This method requires us to find matches using a global matcher, which might result in a larger number of possible false positive matches.
     In other words, we can filter matches based on their quality, but if we set the threshold too high our set of matches will be too sparse and we will not have depth information for most of the image.
     If we set the threshold to low, we will probably end up with many inaccurate matches.
     If we can constrain our matches based on the geometry of our scene, we might be able to filter out some of these false matches without sacrificing the overall quality of matches.

**** epipolar geometry :ignore:
     \\
     [[[epipolar notes]]] describes the geometry of a 2 camera setup, or epipolar geometry.
     In a typical stereogram camera setup, the camera centers are separated by a fixed distance.
     The segment between the two cameras' centers is known as the /baseline/.
     The intersection of the baseline with a camera's image plane is known as an /epipole/.
     If the camera plane and the baseline are parallel, the /epipole/ is a point at infinity, though this is typically not the case.

     For some 3D point $P$ visible in both cameras' images, we can define a few more structures.
     The /epipolar plane/ is the plane formed by the 2 camera's centers and the point $P$.
     It is important to notice that this plane contains the baseline and the epipoles.
     The /epipolar line/ is the intersection of the epipolar plane and a camera plane.
     For any choice of $P$, this line will still contain the epipoles.

***** the importance of parallel cameras [0/2] [0%]                  :ignore:
      \\
      If our cameras' image planes are parallel to the baseline, we know the focal length and the baseline, and we have a matching pair of points from each image, we can calculate the depth to that point using a more simple method than triangulation.
      In figure [[fig:pll_epipolar]], we can see that $\triangle{PLR}$ and $\triangle{PE_LE_R}$ are similar.
      We can use the similar triangles in the diagram to find b_l (the distance along the baseline between camera L and our point P), as shown in figure [[eq:disparity]].

#+CAPTION: Diagram of an epipolar setup where both cameras image planes are parallel to the baseline. /L/ and /R/ are the left and right cameras. /P/ is some point visible to both cameras. /b_l/ and /b_r/ are the distance along the baseline to P from camera's /L/ and /R/ respectively. /x_l/ and /x_r/ is the distance along the x-axis of each camera to the projection of /P/. /E_L/ and /E_R/ are the endpoints of the epipolar line intersecting the projection line for P. /f/ is fhe focal length, and /z/ is the depth.
#+NAME: fig:pll_epipolar
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{tikzpicture}
    \coordinate [label={below left:$L$}] (L) at (0, 0);
    \coordinate [label={below left:$L_f$}] (Lf) at (0, 2);
    \coordinate [label={below left:$L_p$}] (Lp) at (0, 10);
  
    \coordinate [label={below right:$R$}] (R) at (10, 0);
    \coordinate [label={above:$P$}] (P) at (7, 10);
    \coordinate [label={above left:$E_L$}] (El) at (1.4, 2);
    \coordinate [label={above right:$E_R$}] (Er) at (9.4, 2);
  
    \draw [very thick] (L) -- (P) -- (R) -- (L);
    \draw [dashed] (P) -- (7, -1);
    \draw [dashed] (L) -- (Lp) -- (P);
    \draw [dashed] (0, 2) -- (11, 2);
    \draw [dashed] (10, 0) -- (11, 0);
  
    \draw [|-|,semithick] (1.4, 1.5) -- node [below] {$b_l - x_l$} (6.95, 1.5);
    \draw [|-|,semithick] (7.05, 1.5) -- node [below] {$b_r - x_r$} (9.4, 1.5);
    \draw [|-|,semithick] (0, -.5) -- node [below] {$b_l$} (6.95, -.5);
    \draw [|-|,semithick] (7.05, -.5) -- node [below] {$b_r$} (10, -.5);
  
    \draw [|-|,semithick] (.5, 9.9) -- node [right] {$z$} (.5, 0.1);
    \draw [|-|,semithick] (10.5, 1.9) -- node [right] {$f$} (10.5, 0.1);
  \end{tikzpicture}
  
  - $L$, $R$: left and right cameras.
  
  - $P$: a point visible in both cameras.
  
  - $b_l, b_r$: distance along the baseline from L and R to P
  
  - $x_l, x_r$: the distance along the x-axis of each camera to the projection of P
  
  - $E_L, E_R$: endpoints of the epipolar line intersecting the projection lines for P
  
  - $f$: focal length
  
  - $z$: depth
#+end_figure

#+CAPTION: Solving for distance along the baseline.
#+NAME: eq:disparity
#+ATTR_LATEX: :options [H]
#+begin_figure latex :file badpngs/bl_from_triangles.png :results file graphics :exports output
      \begin{align}
        \frac{b_l - x_l}{b_l} = \frac{b_r - x_r}{b_r} \\
        b_lb_r - x_lb_r = b_rb_l - x_rb_l \\
        x_lb_r = x_rb_l \\
        b_l + b_r = b \\
        x_lb_l + x_lb_r = x_lb \\
        x_lb_l + x_rb_l = x_lb \\
        (x_l + x_r)b_l = x_lb \\
        b_l = \frac{x_lb}{x_l + x_r}
      \end{align}
#+end_figure

      We can label the value $x_l + x_r$ the disparity $d$.
      Looking now at similar triangles $\triangle{LL_zP}$ and $\triangle{LL_fE_l}$, we can now solve for the depth $z$, as shown in figure [[eq:depthfromdisp]].

#+CAPTION: Solving for depth from disparity.
#+NAME: eq:depthfromdisp
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{align}
   \frac{z}{b_l} = \frac{f}{x_l} \\
    z = \frac{fb_l}{x_l} = \frac{fb}{d}
  \end{align}
#+end_figure

      As this math depends on our cameras' image planes being parallel to the baseline, we will need to define a transformation to /rectify/ the images of cameras that don't adhere to this constraint.

**** TODO estimating external params :ignore:
     If we can find a set of matching points in our images, we can use our points to determine the required transformation to rectify our images.

***** essential and fundamental matrices [0/3] [0%]                  :ignore:
      \\
      First we must introduce the concept of the essential matrix.
      A 3D point $P$ has corresponding points in the image space of each camera, $p$ and $p'$.
      If we know $p$ and we want to find $p'$, we know that it must lie on the epipolar line for $P$.
      To find the epipolar line, we must know the relative position of our cameras defined by rotation matrix $R$ and translation vector $T$ (in our first camera's reference system).
      We assume for simplicity that our cameras have a focal length of 1 and centers at $\langle 0, 0 \rangle$, so $K = K' = I$.
      Based on this, the projection of $p'$ on camera 1's image plane is $Rp' + T$.

      We know that $T$ (the translation vector between the cameras) is our baseline, and therefore must be on the epipolar plane.
      We also know that $Rp' + T$ must also be on the epipolar plane.
      We can find a vector normal to the epipolar plane using the cross product: $T \times (Rp' + T) = T \times Rp'$.
      For any point on the plane, the dot product with this normal vector should be 0.
      We know that $p$ is on the epipolar plane as well, so $p \cdot (T \times Rp')$.
      For 3D vectors we can rewrite our cross product as matrix multiplication: $p^T (T_{\times} R) p'$.
      From this we can derive our definition of the essential matrix, $E = T_{\times} R$.
      For any point in our first image $p$, it's corresponding matching point must lie on the line ($p^T E$).

      This definition of the essential matrix assumed that $K = K' = I$, so we can factor this back into the equation to get our fundamental matrix: $F = K^{-T} E K'^{-1}$.
      This new matrix performs the same function as the essential matrix, but for cameras with varying focal length and center.
      F has a few interesting properties, it only has 8 degrees-of-freedom since its scale doesn't matter, and it has rank 2 since it maps from points to lines.

      Without knowledge of our camera's intrinsic parameters, we might still be able to find the fundamental matrix assuming we can find a sufficient number of independent epipolar lines.
      Given 2 matching points in our images, $p = [u \ v \ 1]$ and $p' = [u' \ v' \ 1]$, figure [[eq:8p]] shows how we can construct a linear constraint.

#+CAPTION: Constructing a linear constraint on the fundamental matrix from a matching pair of features.
#+NAME: eq:8p
#+ATTR_LATEX: :options [H]
#+begin_figure latex
      \begin{align*}
        p^T F p' = 0 \\
        \begin{bmatrix}u & v & 1\end{bmatrix}
        \begin{bmatrix}
          F_{11} & F_{12} & F_{13} \\
          F_{21} & F_{22} & F_{23} \\
          F_{31} & F_{32} & F_{33}
        \end{bmatrix}
        \begin{bmatrix}
          u' \\
          v' \\
          1
        \end{bmatrix} = 0 \\
        \begin{bmatrix}
          (u * F_{11} + v * F_{21} + F_{31}) &
          (u * F_{12} + v * F_{22} + F_{32}) &
          (u * F_{13} + v * F_{23} + F_{33})
        \end{bmatrix}
        \begin{bmatrix}
          u' \\
          v' \\
          1
        \end{bmatrix} = 0 \\
        \begin{bmatrix}
          u'u & v'u & u
          u'v & v'v & v
          u'  & v'  & 1
        \end{bmatrix}
        \begin{bmatrix}
          F_{11} \\
          F_{12} \\
          F_{13} \\
          F_{21} \\
          F_{22} \\
          F_{23} \\
          F_{31} \\
          F_{32} \\
          F_{33}
        \end{bmatrix} = w \cdot f = 0
      \end{align*}
#+end_figure

      With more matches we can build up a matrix $W$ from row vectors $w_i$, such that $Wf=0$.
      We only need 8 matching points to fully define F (since it only has 8 degrees of freedom), but we can use more to deal with potential noise in our matches, finding F with the smallest mean squared error.
      Our resulting estimate for F might have rank 3, while the real F has rank 2, but we can fix this using SVD, as shown in figure [[eq:8psvd]].

#+CAPTION: Correcting the rank of our estimate for the fundamental matrix using SVD.
#+NAME: eq:8psvd
#+ATTR_LATEX: :options [H]
#+begin_figure latex
  \begin{center}
    \[
    \hat{F} = U
    \begin{bmatrix}
      \sigma_1 & 0 & 0 \\
      0 & \sigma_2 & 0 \\
      0 & 0 & \sigma_3
    \end{bmatrix}
    V
    \]

    \[
    F = U
    \begin{bmatrix}
      \sigma_1 & 0 & 0 \\
      0 & \sigma_2 & 0 \\
      0 & 0 & 0
    \end{bmatrix}
    V
    \]
  \end{center}
#+end_figure

      While using more than 8 matches to estimate F will be more resistant to noise, we might need to employ other methods to deal with outliers in our matches.
      One strategy is to use RANSAC: select random samples from our set of matches and find the one that produces the least outliers.
      Another strategy is to find F that minimizes the median of squared error rather than mean squared error.

***** recovering pose from essential matrix :ignore:
      \\
      Given the camera intrinsic matrix, $K$, we can recover the essential matrix using our estimate of the fundamental matrix.
      Alternatively, we can use a similar algorithm, the 5 point algorithm, to solve for the essential matrix directly.
      Once we have the essential matrix, [[[recover pose]]] describes a method we can use to recover the pose information of our cameras.

***** triangulation :ignore:

***** doing a better job with bundle adjustment :ignore:
      \\
      After we have estimates for our camera's parameters, we can refine them using a process known as bundle adjustment [[[bundle adjustment]]].
      This process uses the parameters for each camera to triangulate our matched points, and then reproject them back to image space.
      The sum of distances between the reprojected points and the actual points is known as reprojection error.
      We can then use a least squares minimizer to minimize this reprojection error.
      This process finds better estimates for camera's intrinsic parameters (e.g. focal length, distortion) and extrinsic parameters (e.g. pose).
      However, as the reprojection error function is not linear, the minimizer is not guaranteed to find the best possible solution.
      In order for bundle adjustment to succeed, we need a reasonable initial estimate for our camera parameters.

**** rectification :ignore:
     \\
     Now that we have camera parameters we can create a transformation to align our image planes with the baseline.
     [[[rectification]]] describes a method for computing this transformation from the fundamental matrix.
     This algorithm breaks down the transformation into 3 component parts: projection, similarity, and shearing.
     The goal of the projection transform is to move the epipoles to a point at infinity.
     The goal of the similarity transform is to rotate the epipoles so they are aligned with the x-axis.
     Finally, the goal of the shearing transform is to preserve the aspect ratio of our images.
     Without the shearing transform, the resulting transformation might squash our image.

**** TODO finding disparity in our rectified image :ignore:
***** stereo block matching :ignore:
      \\
      In our newly rectified image, we now know that our matches need to be approximately aligned horizontally before we consider them.
      This constraint allows us to switch to a different kind of feature matcher: a stereo block matcher.
      In a stereo block matcher, we generate a feature vector for the block around each pixel, and then we find the best match in the same row.
      As not all of these matches will be accurate, the disparity map is then smoothed to make the output less noisy.
      The stereo block matcher is able to do this efficiently by only checking for horizontally allowed matches.
      If we know that points that we care about in our image must be within a certain range of depths, we can use this to constrain where we look for matches as well.
      By using a sliding window to search for matches instead of checking the entire column, a semi-global block matcher is both more efficient and typically has less noise.
***** dealing with untextured areas :ignore:
      \\
      Feature matching in a block matcher still relies on matching images based on their visual appearance.
      This means that large flat, untextured surfaces, such as walls, often produce inaccurate disparity measurements.
      Additionally, at the edges of objects, where blocks include parts of the background, the block matcher might also produce inaccurate results, usually resulting in halo around objects known as /speckle/.
      We can deal with speckle and untextured surfaces using smoothing or filtering, but this will often still result in large areas of inaccurate disparity information.

** Revisiting Related Work
   \\
   We now have all the necessary parts to build a structure from motion pipeline.
   Before we continue, however, it is useful to review the aforementioned work on camera calibration and structure from motion.

*** camera parameters from vanishing points :ignore:
**** fSpy :ignore:
     \\
***** identifiying lines :ignore:
      fSpy uses the geometry of the image to calibrate the camera.
      The user needs find the vanishing points corresponding to 3 pair-wise perpendicular directions.
      They can do this by marking sets of parallel lines on the scene.
      fSpy uses these lines to calculate the vanishing points.
      The dot product of the directions of each pair of vanishing points should be zero, as they are perpendicular:
      
      - [ ] linear equation for the dot product of vanishing points

      These constraints form a system of linear equations, where our coefficients are the coordinates of the vanishing points, and our unknowns are the parts of the camera intrinsic matrix.
      With 3 points we get 3 equations, but the camera intrinsic matrix can have up to 5 unknowns including the 2 focal length values, the principal point, and skew parameter.
      By assuming our image has zero skew and square pixels, we can reduce the number of unknowns to 3.

**** traffic analysis :ignore:
     \\
     The calibration technique in [[[sochor]]] works similarly to fSpy.
***** diamond space :ignore:
      Unlike fSpy, [[[sochor]]] works with more than 2 lines for each vanishing point.
      In order to find the closest point to a set of lines, they use a special dual space known as /Diamond Space/ [[[diamond space]]].
      This dual space simplifies the process of finding the most likely vanishing point, and makes it simpler to group lines into perpendicular groups.
***** first vanishing point :ignore:
     For the first vanishing point, they assume their video frames will have cars moving in parallel lines, following the lanes.
     They first filter only moving features in the video, and then match the features between frames to find several motion vectors throughout the video.
     They then convert the set of lines to diamond space in order to locate the first vanishing point.
***** second vanishing point :ignore:
     For the second vanishing point they also rely on the geometry of moving vehicles.
     They find edges along the cars that remain parallel as the car moves.
     By converting these lines to their dual in diamond Space, they can filter out the edges that aren't perpendicular to the first vanishing point.
     After this they find the vanishing point as they did for the first set.
***** solving for camera intrinsic parameters and the the third vanishing point :ignore:
      Instead of finding a third vanishing point in their scene, they reduce the number of unknowns in the camera intrinsic matrix to just one, by assuming the principal point of each frame is the center of the image.
      By setting up the same equation as fSpy for the first two vanishing points, they can solve the linear equation for the focal length.
**** issues :ignore:
***** scene geometry :ignore:
      \\
     As we mentioned before, both of these techniques rely on having good geometry in the images, which our target historical stereogram does not have.
     Even the "motion" vectors between our camera frames will likely not be parallel as the cameras' pose likely includes some relative rotation.
***** distortion :ignore:
     This process doesn't work for images with significant distortion, and it is unclear whether or not the cameras that took our target images would satisfy this constraint
     If we try to account for distortion in this process, it would no longer be a linear system of equations.
     Bundle adjustment attempts to solve a similar problem for a non-linear system of equations, but it requires us to have a reasonable initial guess in order to avoid falling into a local minima.

* TODO Our Pipeline
* TODO Testing
* Conclusions
** TODO review our design
** why we will meet our AC
*** our pipeline accounts for possible error in our input sources
*** our tests allow us to tune our hyperparameters

* Sources
  1. <<fspy>> https://fspy.io/basics/
  2. <<sochor>> http://www.itspy.cz/wp-content/uploads/2014/11/acmspy2014_submission_25.pdf#page=64&zoom=100,130,908
  3. <<openmvg>> http://imagine.enpc.fr/~moulonp/openMVG/
  4. <<opencv>> https://opencv.org/
  5. <<numpy>>
  6. <<scipy>>
  7. <<camera model notes>> https://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdf
  8. <<homogeneous coords notes>> https://web.stanford.edu/class/cs231a/course_notes/02-single-view-metrology.pdf
  9. <<epipolar notes>> https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf
  10. <<epnp>> Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision, 81(2):155–166, 2009.
  11. <<3pnp>> Xiao-Shan Gao, Xiao-Rong Hou, Jianliang Tang, and Hang-Fei Cheng. Complete solution classification for the perspective-three-point problem. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(8):930–943, 2003.
  12. <<recover pose>> https://www.uio.no/studier/emner/matnat/its/nedlagte-emner/UNIK4690/v16/forelesninger/lecture_7_3-pose-from-epipolar-geometry.pdf
  13. <<bundle adjustment>> http://lear.inrialpes.fr/pubs/2000/TMHF00/Triggs-va99.pdf
  14. <<rectification>> http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document2_Loop-Zhang-CVPR1999.pdf
  15. <<diamond space>> http://www.fit.vutbr.cz/research/groups/graph/pclines/papers/2013-BMVC-Dubska-VanishingPointsDetection.pdf
  16. <<structure from motion>> https://www.mathworks.com/help/vision/ug/structure-from-motion.html
  17. <<loc>> https://www.loc.gov/pictures/item/2018671309/
