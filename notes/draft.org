* Technical Objectives
** Input
*** focal length
    - From the focal length estimator, we expect to recieve a rough estimate of the focal length.
    - This estimate is expected to have some error, so further refinement might be required to achieve a useful depth estimate.
*** stereograms
    - Pair of input images.
    - As these are scans of historical photos, these are expected to be quite noisy.
    - The position of the photo centers is also expected to vary between the 2 images.
    - The images in this case are not cropped, so there will be additional noise around the edges.
    - These images might have some degree of additional distortion, which we will not know ahead of time.
*** additional ground truth data
    - This data will provide a pair of points in one of the images, as well as a required scale.
    - Ideally, this will be used to establish the scale of the output depth map.
*** what we won't have
    - accurate camera intrinsics (distortion) or external parameters (camera's relative pose)
      
** Output
*** Real-world coordinates
    - for each pixel relative to the first camera's reference frame

** Acceptance Criteria
*** Accuracy within 30 cm (~ one foot) for any visible object in the image
    - Maximum distance for this accuracy measurment will be to the last visible pyramid in the image.
*** Reasonable accuracy for other (probably non-historical) images from a test dataset

* Similar Work
** Refining camera parameters using parallel lines
*** fSpy
**** I/O
     - Takes in user-inputted set of pairs of parallel lines in the image, which should be perpendicular
     - Outputs a focal length estimation
**** why we can't use it
     - Requires the structure of the scene to have ample perpendicular lines. While this is typical of a cityscape, this isn't exactly the case for our gravesite photo.
     - Only does the work of refining focal length estimates, we would still need to build the rest of the pipeline.
*** Camera calibration technique from Traffic Analysis From Video (Autor Prace)
**** I/O
     - Takes in video frames from traffic cam
     - Uses parallel lines produced by moving traffic to find the vanishing points and then the focal length.
**** why we can't use it
     - While we do have multiple frames and can draw motion vectors between matching features in each frame, our left and right cameras are likely rotated so the lines between features will likely not be parallel

** TODO existing Structure from motion pipelines and libraries
   Many tools exist to perform the different parts of the SfM pipeline, but pre-made solutions are made to solve slightly different problems (many video frames, little noise, no distortion)
*** OpenMVG
**** features
     OpenMVG provides tools for multiple view geometry calculations, including structure from motion and PnP.
**** issues
     Only provides C++ bindings, so we would need to provide our own.
*** OpenCV
**** features
     OpenCV does not provide many features specifically related to epipolar geometry, but it does provide the tools we need to build most of the parts of the pipeline.
     OpenCV also has Python bindings, allowing us to avoid wrapping / distributing native binaries ourselves.
**** issues
     Doesn't provide a method for performing bundle adjustment
*** scipy
**** features
     While not built specifically for computer vision, scipy provides useful functions for optimization, that we can use to implement bundle adjustment or the 8-point algorithm.

* Modelling The Problem
** the camera
   - In order to understand our problem, we need a good model for the camera
*** pinhole camera model
**** pinhole
     - origin of the camera's coordinate system
**** aperture
     - the place our image will be projected, "image space"
     - focal length: distance from the pinhole / origin to the aperature
**** optical axis:
     - line normal to the aperture passing through the origin
**** principal point:
     - the intersection point of the optical axis and the aperture
     - the projection of the pinhole / origin in image space
*** pinhole camera model approximates real cameras
    - real camera's use lenses, and lenses don't necessarily have fixed focal length
**** distortion
     - caused by variations in lens's focal length
     - radial distortion (pinchusion, barrel): results in variation in focal length as we move away from the center of the lens
     - if distortion is significant, we will need to account for it before we can get accurate measurements
*** getting from some 3D coordinate system to image space
**** extrinsic properties / pose
     - some coordinate space to camera coordinate space
     - useful to describe rotation between our 2 cameras
**** intrinsic properties / projection
     - describe the center of the camera and focal length
     - projects from our camera's reference coordinate system to images space

** mathematical model
   - at this point we need a way to describe our camara's parameters using math
*** TODO homogenous coordinates
    - it is important to introduce a new coordinate system so that we can describe more types of transformations
*** TODO projection matrix 
    - allows us to represent the projection part of the transformation from 3D coordinates to 2D homogenous coordinates
*** TODO adding pose information
    - the extrinsic parameters of the camera may include some rotation and translation mapping from some 3D coordinate system (e.g. the reference coords of the left camera) to the current camera's coordiante system (e.g. the coordinate system of the right camera)
    - by default our projection matrix includes no rotation and no translation, so we can write it like this [TODO]
    - we can incorporate some rotation, R, and translation vector, T, like this
    - note that because of the ordering of matrix operations this is the rotation and translation of the points, not the camera. the corresponding camera pose would be R^T and -R ^ T \cdot T
   
** resolving undefined depth using structure from motion
*** depth with a single camera
  - looking at the model we have so far, we can see that depth has the effect of moving our points towards the principal point in image space
  - using this information, if we know the size of an object (at the angle it is being viewed by the camera), and the focal length, we can determine the size of the object
    - however in cases where we don't have this information (such as when we don't have the object's measurements), we won't be able to determine the depth with the information provided by a single camera
**** PnP
    - in cases where we know an objects exact dimensions, there are a few methods we can use to determine it's pose to the camera
***** pose from pairs of parallel lines
      - in cases where the objects geometry is simple, we can [insert stuff about normal of plane from vanishing points]
***** TODO pose in the general case
      - insert stuff about perspective-n-points
      
*** TODO depth with 2 cameras
    - with a pair of cameras, we can use the extra information provided by the second camera to resolve this issue of depth, provided we know the relative pose of the other camera, and that we can identify the same point in both images
    - the geometry of a 2 camera setup is known as epipolar geometry, and the general problem of resolving depth / location information using 2 or more camera frames (maybe a video, maybe a stereo setup like we have) is known as structure from motion
   
** how existing tools use this math to build their pipelines
*** 

* Our Pipeline

* Testing

* Conclusions
** review our design
** why we will meet our AC
*** our pipeline accounts for possible error in our input sources
*** our tests allow us to tune our hyperparameters

* Sources
** https://fspy.io/basics/
** http://www.itspy.cz/wp-content/uploads/2014/11/acmspy2014_submission_25.pdf#page=64&zoom=100,130,908

