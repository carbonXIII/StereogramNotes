* Technical Objectives
** Input
*** focal length
    - From the focal length estimator, we expect to recieve a rough estimate of the focal length.
    - This estimate is expected to have some error, so further refinement might be required to achieve a useful depth estimate.
*** stereograms
    - Pair of input images.
    - As these are scans of historical photos, these are expected to be quite noisy.
    - The position of the photo centers is also expected to vary between the 2 images.
    - The images in this case are not cropped, so there will be additional noise around the edges.
    - These images might have some degree of additional distortion, which we will not know ahead of time.
*** additional ground truth data
    - This data will provide a pair of points in one of the images, as well as a required scale.
    - Ideally, this will be used to establish the scale of the output depth map.
*** what we won't have
    - accurate camera intrinsics (distortion) or external parameters (camera's relative pose)
      
** Output
*** Real-world coordinates
    - for each pixel relative to the first camera's reference frame

** Acceptance Criteria
*** Accuracy within 30 cm (~ one foot) for any visible object in the image
    - Maximum distance for this accuracy measurment will be to the last visible pyramid in the image.
*** Reasonable accuracy for other (probably non-historical) images from a test dataset

* Similar Work
** Refining camera parameters using parallel lines
*** fSpy
**** I/O
     - Takes in user-inputted set of pairs of parallel lines in the image, which should be perpendicular
     - Outputs a focal length estimation
**** why we can't use it
     - Requires the structure of the scene to have ample perpendicular lines. While this is typical of a cityscape, this isn't exactly the case for our gravesite photo.
     - Only does the work of refining focal length estimates, we would still need to build the rest of the pipeline.
*** Camera calibration technique from Traffic Analysis From Video (Autor Prace)
**** I/O
     - Takes in video frames from traffic cam
     - Uses parallel lines produced by moving traffic to find the vanishing points and then the focal length.
**** why we can't use it
     - While we do have multiple frames and can draw motion vectors between matching features in each frame, our left and right cameras are likely rotated so the lines between features will likely not be parallel

** existing Structure from motion pipelines and libraries
   Many tools exist to perform the different parts of the SfM pipeline, but pre-made solutions are made to solve slightly different problems (many video frames, little noise, no distortion)
*** OpenMVG
**** features
     OpenMVG provides tools for multiple view geometry calculations, including structure from motion and PnP.
**** issues
     Only provides C++ bindings, so we would need to provide our own.
*** OpenCV
**** features
     OpenCV does not provide many features specifically related to epipolar geometry, but it does provide the tools we need to build most of the parts of the pipeline.
     OpenCV also has Python bindings, allowing us to avoid wrapping / distributing native binaries ourselves.
**** issues
     Doesn't provide a method for performing bundle adjustment
*** scipy
**** features
     While not built specifically for computer vision, scipy provides useful functions for optimization, that we can use to implement bundle adjustment or the 8-point algorithm.

* Modelling The Problem
** the camera
   - In order to understand our problem, we need a good model for the camera
*** pinhole camera model
**** pinhole
     - origin of the camera's coordinate system
**** aperture
     - the place our image will be projected, "image space"
     - focal length: distance from the pinhole / origin to the aperature
**** optical axis:
     - line normal to the aperture passing through the origin
**** principal point:
     - the intersection point of the optical axis and the aperture
     - the projection of the pinhole / origin in image space
*** pinhole camera model approximates real cameras
    - real camera's use lenses, and lenses don't necessarily have fixed focal length
**** distortion
     - caused by variations in lens's focal length
     - radial distortion (pinchusion, barrel): results in variation in focal length as we move away from the center of the lens
     - if distortion is significant, we will need to account for it before we can get accurate measurements
*** getting from some 3D coordinate system to image space
**** extrinsic properties / pose
     - some coordinate space to camera coordinate space
     - useful to describe rotation between our 2 cameras
**** intrinsic properties / projection
     - describe the center of the camera and focal length
     - projects from our camera's reference coordinate system to images space

** mathematical model
   - at this point we need a way to describe our camara's parameters using math
*** TODO homogenous coordinates
    - it is important to introduce a new coordinate system so that we can describe more types of transformations
*** TODO projection matrix 
    - allows us to represent the projection part of the transformation from 3D coordinates to 2D homogenous coordinates
*** TODO adding pose information [0/1] [0%]
    - the extrinsic parameters of the camera may include some rotation and translation mapping from some 3D coordinate system (e.g. the reference coords of the left camera) to the current camera's coordiante system (e.g. the coordinate system of the right camera)
    - [ ] TODO: by default our projection matrix includes no rotation and no translation, so we can write it like this
    - we can incorporate some rotation, R, and translation vector, T, like this
    - note that because of the ordering of matrix operations this is the rotation and translation of the points, not the camera. the corresponding camera pose would be R^T and -R ^ T \cdot T
   
** resolving undefined depth using structure from motion
*** depth with a single camera
  - looking at the model we have so far, we can see that depth has the effect of moving our points towards the principal point in image space
  - using this information, if we know the size of an object (at the angle it is being viewed by the camera), and the focal length, we can determine the size of the object
    - however in cases where we don't have this information (such as when we don't have the object's measurements), we won't be able to determine the depth with the information provided by a single camera
**** PnP
    - in cases where we know an objects exact dimensions, there are a few methods we can use to determine it's pose to the camera
***** pose from pairs of parallel lines
      - in cases where the objects geometry is simple, we can [insert stuff about normal of plane from vanishing points]
***** TODO pose in the general case
      - insert stuff about perspective-n-points
      
*** TODO depth with 2 cameras
    - with a pair of cameras, we can use the extra information provided by the second camera to resolve this issue of depth, provided we know the relative pose of the other camera, and that we can identify the same point in both images
    - the geometry of a 2 camera setup is known as epipolar geometry, and the general problem of resolving depth / location information using 2 or more camera frames (maybe a video, maybe a stereo setup like we have) is known as structure from motion
**** TODO estimating external params
***** essential and fundamental matrices
***** doing a better job with bundle adjustment
**** TODO rectification
**** TODO finding disparity in our rectified image
***** stereo block matching
***** dealing with untextured areas
   
** how existing tools use this math to build their pipelines
*** camera parameters from vanishing points
**** fSpy
***** identifiying lines
     - based on the geometry of the image, the user inputs 2 pairs of parallel lines, where each set should be perpendicular in 3D space
     - this step can technically be automated using Hough lines and guessing perpendicular lines, but knowledge of the objects in the image can allow the user to avoid errors that this blind process might make
***** vanishing points
     - for each pair of parallel lines, we can find the vanishing points (their intersections in image space).
     - using the inverse projection matrix this vanishing points could be resolved into 3D directions from the origin of the camera space
***** solving for camera intrinsic parameters
     - as we know the pairs of parallel lines are perpendicular to each other, we know that the corresponding directions in camera space should be perpendicular
     - their dot product should therefore be equal to 0
     - we can use this to setup a linear system of equations, our variables derived from the parts of the camera intrinsic matrix, and our coefficients derived from the coordinates of the vanishing points
     - the camera intrinsic matrix has 3 degrees of freedom (if assume zero skewness and square pixels), so we need 3 pairs of points to fully define this
**** traffic analysis
     - the calibration in this paper works similarly to fSpy
***** diamond space
      - special dual space for lines that allows them to easily find the most common intersection of a set of lines by finding the global maximum
***** first vanishing point
     - they assume their video frames will have cars moving in parallel lines, following the lanes
     - they first filter only moving features in the video, and then match the features between frames to find several motion vectors throughout the video
     - they then (using diamond space) find the first vanishing point
***** second vanishing point
     - again using features from moving cars in the video frame, they find edges along the cars that remain parallel as the car moves
     - these edges should be perpendicular to the edges from the first vanishing point
***** solving for camera intrinsic parameters and the the third vanishing point
      - by assuming the principal point of the camera is at the center of the image, they can solve the linear equations for the focal length and the third vanishing point
**** issues
***** scene geometry
     - our scene doesn't have good geometry for the extraction of vanishing points
     - even the "motion" vectors between our camera frames will likely not be parallel as the cameras are likely relatively rotated
***** distortion
     - this process doesn't work for images with significant distortion, and it is unclear whether or not the cameras that took our target images would satisfy this constraint
     - if we try to account for distortion in this process, it would no longer by a linear system of equations 

* TODO Our Pipeline

* TODO Testing

* Conclusions
** TODO review our design
** why we will meet our AC
*** our pipeline accounts for possible error in our input sources
*** our tests allow us to tune our hyperparameters

* Sources
** https://fspy.io/basics/
** http://www.itspy.cz/wp-content/uploads/2014/11/acmspy2014_submission_25.pdf#page=64&zoom=100,130,908
** https://web.stanford.edu/class/cs231a/course_notes/02-single-view-metrology.pdf

