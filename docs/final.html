<h1 id="technical-objectives">Technical Objectives</h1>
<p>The primary function of the depth estimation algorithm is to take in a predicted focal length, and a pair of stereogram images and to produce an RGBD (Red, Green, Blue, and Depth for each pixel) image.</p>
<p>The focal length estimator should produce some confidence interval of focal lengths. This focal length is assumed to be roughly the same for both cameras. We also expect to have 2 matching input stereograms, taken from cameras a reasonable distance apart, rotated to focus on a single focus target. These stereograms will most likely be somewhat noisy. There may also be slight variations in rotation, position, or scale between the two images (as they will most likely be scans of physical stereograms). There may also be some distortion due to inconsistencies in the focal length of the lense. This distortion is assumed to be negligible.</p>
<p>All of these different sources of error must be taken into account by the depth finding algorithm, and it must still be able to produce an accurate RGBD image. In later stages, this RGBD image will be used to find the distance between points in the image. We require the RGBD image to be accurate enough such that the measured distance is within one foot of the real distance.</p>
<h1 id="research">Research</h1>
<p>As our goal is to determine depth from a stereogram image, it is useful to first consider how humans perceive depth as a stereogram camera setup is modeled after the human eyes. One easy observation, is that objects appear smaller as they move further away. This is helpful, as if we know the size of an object we can easily determine it's distance using only one eye. However, in some circumstances where the sizes of objects are unknown this is not possible. In these cases, being able to see the same scene from 2 different angles at once can help to resolve the depth of unknown objects. In our depth finding algorithm, we do not know the exact size of many of the objects in our image, so it is necessary to use the information from both images in order to determine the depth of objects in our scene.</p>
<p></p>
<p>In order to talk more concretely about how a stereogram camera setup mirrors the human eyes, we need the correct mathematical tools. Homogeneous coordinates provide an extremely useful way to build upon the concepts of linear algebra to describe more types of transformations. For this first explanation of homogeneous coordinates, we will focus on 2D, but most of the concepts presented here can be easily extended to 3D as well.</p>
<p></p>
<p>In traditional cartesian coordinates, a point in 2D is described by a vector with 2 coordinates, such as: <span class="math inline">⟨<em>x</em>, <em>y</em>⟩</span>. In homogeneous coordinates, this same point can be represented by any vector of the form <span class="math inline">⟨<em>x</em><em>Z</em>, <em>y</em><em>Z</em>, <em>Z</em>⟩</span>, where <span class="math inline"><em>Z</em></span> is a non-zero real number. This new coordinate, <span class="math inline"><em>Z</em></span>, acts as a normalization factor: scaling our vector by any non-zero scaling factor yields the same point. To convert back to cartesian coordinates, we only need to divide by the scaling factor.</p>
<p></p>
<p>What happens when our normalization factor is zero? Intuitively, when we take a point of the form <span class="math inline">⟨<em>x</em>, <em>y</em>, 0⟩</span> and we divide by the scaling factor, we end up with our <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> coordinates at positive or negative infinity. Therefore a vector of the form <span class="math inline">⟨<em>x</em>, <em>y</em>, 0⟩</span> can be seen to represent a point infinitely far away in the direction <span class="math inline">⟨<em>x</em>, <em>y</em>⟩</span> or <span class="math inline">⟨ − <em>x</em>,  − <em>y</em>⟩</span>. We will call this concept a point at infinity. If we extend this concept to higher dimensions, we can also have lines or planes at infinity, following a similar format.</p>
<p></p>
<p>Lines are described using the same format <span class="math inline">⟨<em>a</em>, <em>b</em>, <em>c</em>⟩</span>. In this case, <span class="math inline"> − <em>a</em>/<em>b</em></span> is the slope, and <span class="math inline"> − <em>c</em>/<em>b</em></span> is the y-intercept. To determine if some point is on a line, we use the dot product. If <span class="math inline"><em>p</em> ⋅ <em>l</em> = 0</span>, the point <span class="math inline"><em>p</em></span> is on the line <span class="math inline"><em>l</em></span>. From this we can see that the vector <span class="math inline">⟨<em>a</em>, <em>b</em>, 1⟩</span> is normal to the line <span class="math inline">⟨<em>a</em>, <em>b</em>, <em>c</em>⟩</span>. The intersection of 2 lines, α and β, is α × β. If the lines are parallel, this intersection will be a point at infinity in the same direction as the lines.</p>
<p></p>
<p> <img src="svgs/cross_prod.svg" /> </p>
<p></p>
<p>How do these coordinates help us to model the camera?</p>
