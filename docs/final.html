<h1 id="technical-objectives">Technical Objectives</h1>
<p>The primary function of the depth estimation algorithm is to take in a predicted focal length, and a pair of stereogram images and to produce an RGBD (Red, Green, Blue, and Depth for each pixel) image.</p>
<p>The focal length estimator should produce some expected of focal lengths. This focal length is assumed to be roughly the same for both cameras. We also expect to have 2 matching input stereograms, taken from cameras a reasonable distance apart, rotated to focus on a single focus target. These stereograms will most likely be somewhat noisy. There may also be slight variations in rotation, position, or scale between the two images (as they will most likely be scans of physical stereograms). There may also be some distortion due to inconsistencies in the focal length of the lense. This distortion is assumed to be negligible.</p>
<p>All of these different sources of error must be taken into account by the depth finding algorithm, and it must still be able to produce an accurate RGBD image. In later stages, this RGBD image will be used to find the distance between points in the image. We require the RGBD image to be accurate enough such that the measured distance is within one foot of the real distance.</p>
<h1 id="research">Research</h1>
<p>As our goal is to determine depth from a stereogram image, it is useful to first consider how humans perceive depth as a stereogram camera setup is modeled after the human eyes. One easy observation is that objects appear smaller as they move further away. This is helpful, as if we know the size of an object we can easily determine it's distance using only one eye. However, in some circumstances where the sizes of objects are unknown this is not possible. In these cases, being able to see the same scene from 2 different angles at once can help to resolve the depth of unknown objects. In our depth finding algorithm, we do not know the exact size of many of the objects in our image, so it is necessary to use the information from both images in order to determine the depth of objects in our scene.</p>
<p>In order to talk more concretely about how a stereogram camera setup mirrors the human eyes, we need the correct mathematical tools. Homogeneous coordinates provide an extremely useful way to build upon the concepts of linear algebra to describe more types of transformations. For this first explanation of homogeneous coordinates, we will focus on 2D, but most of the concepts presented here can be easily extended to 3D as well.</p>
<p>In cartesian coordinates, a point in 2D is described by a vector with 2 coordinates, such as: <span class="math inline">⟨<em>x</em>, <em>y</em>⟩</span>. In homogeneous coordinates, this same point can be represented by any vector of the form <span class="math inline">⟨<em>x</em><em>Z</em>, <em>y</em><em>Z</em>, <em>Z</em>⟩</span>, where <span class="math inline"><em>Z</em></span> is a non-zero real number. This new coordinate, <span class="math inline"><em>Z</em></span>, acts as a normalization factor: scaling our vector by any non-zero scaling factor yields the same point. To convert back to cartesian coordinates, we only need to divide by the scaling factor.</p>
<p>What happens when our normalization factor is zero? Intuitively, when we take a point of the form <span class="math inline">⟨<em>x</em>, <em>y</em>, 0⟩</span> and we divide by the scaling factor, we end up with our <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> coordinates at positive or negative infinity. Therefore a vector of the form <span class="math inline">⟨<em>x</em>, <em>y</em>, 0⟩</span> can be seen to represent a point infinitely far away in the direction <span class="math inline">⟨<em>x</em>, <em>y</em>⟩</span> or <span class="math inline">⟨ − <em>x</em>,  − <em>y</em>⟩</span>. We will call this concept a point at infinity. If we extend this concept to higher dimensions, we can also have lines or planes at infinity, following a similar format.</p>
<p>Lines are described using the same format <span class="math inline">⟨<em>a</em>, <em>b</em>, <em>c</em>⟩</span>. In this case, <span class="math inline"> − <em>a</em>/<em>b</em></span> is the slope, and <span class="math inline"> − <em>c</em>/<em>b</em></span> is the y-intercept. To determine if some point is on a line, we use the dot product. If <span class="math inline"><em>p</em> ⋅ <em>l</em> = 0</span>, the point <span class="math inline"><em>p</em></span> is on the line <span class="math inline"><em>l</em></span>. From this we can see that the vector <span class="math inline">⟨<em>a</em>, <em>b</em>, 1⟩</span> is normal to the line <span class="math inline">⟨<em>a</em>, <em>b</em>, <em>c</em>⟩</span>. The intersection of 2 lines, α and β, is α × β. If the lines are parallel, this intersection will be a point at infinity in the same direction as the lines.</p>
<p> <img src="images/cross_prod.png" /> </p>
<p>Transformations in homogeneous coordinates are modelled using matrix operations, just like cartesian coordinates. However, as homogeneous coordinates add an extra normalization factor, these matrices have additional degrees of freedom when compared to a similar matrix in cartesian coordinates.</p>
<p> <img src="images/proj_matrix_breakdown.png" /> </p>
<p>The equation above shows the breakdown of a matrix in homogeneous coordinates, <span class="math inline"><em>P</em></span>, into its component parts. In the 2D case, <span class="math inline"><em>A</em></span> is a 2x2 matrix, <span class="math inline"><em>t</em></span> and <span class="math inline"><em>v</em></span> are vectors, and <span class="math inline"><em>b</em></span> is a scalar. When <span class="math inline"><em>v</em> = 0</span>, and <span class="math inline"><em>b</em> = 1</span>, matrix <span class="math inline"><em>P</em></span> performs the same transformation as the <span class="math inline"><em>A</em></span> followed by some translation by <span class="math inline"><em>t</em></span>, but on homogeneous coordinates. Transformations in cartesian coordinates are limited to affine transformations: transformations that maintain parallelism: lines that were parallel before the transformation remain parallel after it. However, by varying <span class="math inline"><em>v</em></span> in our new matrix <span class="math inline"><em>P</em></span>, we are capable of going one step further: projection. In a projection matrix, parallelism does not have to be preserved. An additional side affect of varying <span class="math inline"><em>v</em></span> is that <span class="math inline"><em>P</em></span> can now map points at infinity to regular points and vice-versa.</p>
<p>Projection matrices are important to modelling the behavior the human eye or a camera. Parallel lines in 3D are often viewed as non-parallel in image space. Additionally, points infinitely far away are still mapped to finite point in image space.</p>
<p>Formally, the coordinates of some point <span class="math inline">⟨<em>x</em>, <em>y</em>, <em>z</em>⟩</span> are mapped to a point in image space <span class="math inline">⟨<em>x</em>′, <em>y</em>′⟩</span>, for some camera with focal length <span class="math inline"><em>f</em></span>, center <span class="math inline">⟨<em>c</em><sub><em>x</em></sub>, <em>c</em><sub><em>y</em></sub>⟩</span>, and no distortion as follows:</p>
<p> <img src="images/cartesian_projection.png" /> </p>
<p>Generally, as <span class="math inline"><em>z</em></span> approaches infinity, the point in image space approaches the center of the camera as we expect. One thing to notice is that this mapping is not linear. However, we can rewrite this using homogeneous coordinates:</p>
<p> <img src="images/homogenous_projection.png" /> </p>
<p>One can verify this result is equivalent to the cartesian version by normalizing the vector. In contrast to the projection in cartesian coordinates, this transformation is linear, so we can rewrite it using a projection matrix. We can further refine this definition by pulling out the intrinsic camera matrix:</p>
<p> <img src="images/intrinsic_matrix.png" /> </p>
<p>Finally, we can add in the camera's rotation <span class="math inline"><em>R</em></span> and translation <span class="math inline"><em>T</em></span> relative to the origin (i.e. the camera's extrinsic parameters) to construct a final projection matrix <span class="math inline"><em>M</em> = <em>K</em> [<em>R</em> <em>T</em>]</span>. This does not reflect the camera's distortion, but we are currently assuming the distortion of input images to be negligible.</p>
<p>We can use this camera model to find the angle of lines relative to our camera. Given two lines that are parallel in 3D space, their intersection is a point at infinity, <span class="math inline"><em>x</em><sub>∞</sub></span>. Using our projection matrix to map this to a point in image space, <span class="math inline"><em>p</em><sub>∞</sub></span>, we will find that this point is no longer a point at infinity. <span class="math inline"><em>p</em><sub>∞</sub></span> is known as the <em>vanishing point</em> of our lines. By finding the vanishing point of parallel lines in image space, we can find the angle of those lines in 3D space: <span class="math inline"><em>d</em> = <em>K</em><sup>−1</sup> ⋅ <em>p</em><sub>∞</sub></span>, where <span class="math inline"><em>d</em></span> is a vector in cartesian coordinates in the direction of our lines. Extending this further to planes, if we identify 2 co-planar pairs of parallel lines in our image, we can find vanishing points in our image for each of them. The line formed by these vanishing points is known as the vanishing line or horizon line, and is unique to the angle of the plane. If we apply our intrinsic camera matrix again we can calculate the normal of the plane in cartesian coordinates, <span class="math inline"><em>n</em> = <em>K</em><sup>−1</sup> ⋅ <em>l</em><sub>∞</sub></span>.</p>
<p>If our scene has the correct geometry, we can use this relationship to find the focal length of the scene. This requires our scene to have 3 vanishing points, where all the sets of directions used must be perpendicular. Existing programs such as fSpy use this method by having the user input the vanishing points by hand, and then using them to calculate the focal length. This can be potentially automated by using the Hough Transform to identify clusters of parallel lines in a scene. While this is generally the case for pictures of architecture, our target image is a graveyard and lacks the required geometry for this method of determining the focal length to work.</p>
<p>How can we use this model to determine depth? One possibility is to use the size of an object and the camera's focal length to determine the depth. This requires us to know the rotation of the camera relative to the camera, as well as the size of the object. In our image there are a few reference objects (e.g. they pyramids) that we know the size of, and these objects might have the necessary geometry to determine their rotation using their vanishing points. However, many of the objects in our image have unknown dimensions, or do not have the right geometry to determine their rotation relative to the camera. In order to solve this problem in the general case where we don't know the size of objects in our image, we need to take advantage of the information provided by the second camera.</p>
<p>First, it is important to describe the geometry of this setup. In a typical stereogram camera setup, the camera centers are separated by a fixed distance. The segment between the two cameras' centers is known as the <em>baseline</em>. The intersection of the baseline with a camera's image plane is known as an <em>epipole</em>. If the camera plane and the baseline are parallel, the <em>epipole</em> is a point at infinity, though this is typically not the case.</p>
<p>For some 3D point <span class="math inline"><em>P</em></span> in both cameras' image spaces, we can define a few more structures. The <em>epipolar plane</em> is the plane formed by the 2 camera's centers and the P. It is important to notice that this plane contains the baseline and the epipoles. The <em>epipolar line</em> is the intersection of the epipolar plane and a camera plane. For any choice of <span class="math inline"><em>P</em></span>, this line will still contain the epipoles.</p>
<p> <img src="images/diagram_triangle.png" /> </p>
<p>If our cameras' image planes are parallel to the baseline, we know the focal length and the baseline, and we have a matching pair of points from each image, we can calculate the depth to that point. Using the diagram above, we can see that △<em>P</em><em>L</em><em>R</em> and △<em>P</em><em>E</em><sub><em>L</em></sub><em>E</em><sub><em>R</em></sub> are similar. We can use the similar triangles in the diagram to find b<sub>l</sub> (the distance along the baseline between camera L and our point P):</p>
<p> <img src="images/bl_from_triangles.png" /> </p>
<p>We can name the value <span class="math inline"><em>x</em><sub><em>l</em></sub> + <em>x</em><sub><em>r</em></sub></span> the disparity <span class="math inline"><em>d</em></span>. Looking now at similar triangles △<em>L</em><em>L</em><sub><em>z</em></sub><em>P</em> and △<em>L</em><em>L</em><sub><em>f</em></sub><em>E</em><sub><em>l</em></sub>, we can finally get the depth <span class="math inline"><em>z</em></span>:</p>
<p> <img src="images/z_from_triangles.png" /> </p>
<p>As this math depends on our cameras' image planes being parallel to the baseline, we will need to define a homography to <em>rectify</em> the images of cameras that don't adhere to this constraint. We will need to find the <em>fundamental matrix</em> in order to do this.</p>
<p>A 3D point <span class="math inline"><em>P</em></span> has corresponding points in the image space of each camera, <span class="math inline"><em>p</em></span> and <span class="math inline"><em>p</em>′</span>. If we know <span class="math inline"><em>p</em></span> and we want to find <span class="math inline"><em>p</em>′</span>, we know that it must lie on the epipolar line for <span class="math inline"><em>P</em></span>. To find the epipolar line, we must know the relative position of our cameras defined by rotation matrix <span class="math inline"><em>R</em></span> and translation vector <span class="math inline"><em>T</em></span> (in our first camera's reference system). We assume for simplicity that our cameras have a focal length of 1 and centers at <span class="math inline">⟨0, 0⟩</span>, so <span class="math inline"><em>K</em> = <em>K</em>′ = <em>I</em></span>. Based on this, the projection of <span class="math inline"><em>p</em>′</span> on camera 1's image plane is <span class="math inline"><em>R</em><em>p</em>′ + <em>T</em></span>.</p>
<p>We know that <span class="math inline"><em>T</em></span> (the translation vector between the cameras) is our baseline, and therefore must be on the epipolar plane. We also know that <span class="math inline"><em>R</em><em>p</em>′ + <em>T</em></span> must also be on the epipolar plane. We can find a vector normal to the epipolar plane using the cross product: <span class="math inline"><em>T</em> × (<em>R</em><em>p</em>′+<em>T</em>) = <em>T</em> × <em>R</em><em>p</em>′</span>. For any point on the plane, the dot product with this normal vector should be 0. We know that <span class="math inline"><em>p</em></span> is on the epipolar plane as well, so <span class="math inline"><em>p</em> ⋅ (<em>T</em>×<em>R</em><em>p</em>′)</span>. For 3D vectors we can rewrite our cross product as matrix multiplication: <span class="math inline"><em>p</em><sup><em>T</em></sup>(<em>T</em><sub>×</sub><em>R</em>)<em>p</em>′</span>. From this we can derive our definition of the essential matrix, <span class="math inline"><em>E</em> = <em>T</em><sub>×</sub><em>R</em></span>. For any point in our first image <span class="math inline"><em>p</em></span>, (<span class="math inline"><em>p</em><sup><em>T</em></sup><em>E</em></span>) is a line where our matching point must be.</p>
<p>This definition of the essential matrix assumed that <span class="math inline"><em>K</em> = <em>K</em>′ = <em>I</em></span>, so we can factor this back into the equation to get our fundamental matrix: <span class="math inline"><em>F</em> = <em>K</em><sup>−<em>T</em></sup><em>E</em><em>K</em>′<sup>−1</sup></span>. This new matrix performs the same function as the essential matrix, but for cameras with varying focal length and center. F has a few interesting properties, it only has 8 degrees-of-freedom since its scale doesn't matter, and it has rank 2 since it maps from points to lines.</p>
<p>Without knowledge of our camera's intrinsic parameters, we might still be able to find the fundamental matrix assuming we can find a sufficient number of independent epipolar lines. Given 2 matching points in our images, <span class="math inline"><em>p</em> = ⟨<em>u</em>, <em>v</em>, 1⟩</span> and <span class="math inline"><em>p</em>′ = ⟨<em>u</em>′, <em>v</em>′, 1⟩</span>:</p>
<p> <img src="images/eight_point.png" /> </p>
<p>With more matches we can build up a matrix <span class="math inline"><em>W</em></span> from row vectors <span class="math inline"><em>w</em><sub><em>i</em></sub></span>, such that <span class="math inline"><em>W</em><em>f</em> = 0</span>. We only need 8 matching points to fully define F (since it only has 8 degrees of freedom), but we can use more to deal with potential noise in our matches, finding F with the smallest mean squared error. Our resulting estimate for F might have rank 3, while the real F has rank 2, but we can fix this using SVD:</p>
<p> <img src="images/correcting_F_rank.png" /> </p>
<p>While using more than 8 matches to estimate F will be more resistant to noise, we might need to employ other methods to deal with outliers in our matches. One strategy is to use RANSAC: select random samples from our set of matches and find the one that produces the least outliers. Another strategy is to find F that minimizes the median of squared error rather than mean squared error.</p>
<p>There are several methods to find our initial set of matches in order to estimate the fundamental matrix. One robust method is by using a feature detector. In the first step, we can run feature detection on the first image, which will return a list of potentially good features (usually corners) and a descriptor generated by sampling the feature's neighboring pixels. After we have this set of descriptors in the first image, we can search for matches in the second image, comparing the feature descriptors in order to predict whether or not a pair of features is the same. This matching is usually somewhat noisy, and might generate impossible matches that it might not have if images were known to be properly aligned.</p>
<p>We can use our fundamental matrix to rectify our images to allow us to generate depth from disparity, using the formulas we constructed earlier. We can do this by using the algorithm laid out by Loop and Zhang to construct a matrix to perform this transformation. The algorithm breaks the matrix down into 3 components: projection, similarity, and shearing. The projection component handles transforms our image plane to be parallel to the baseline. This maps the epipoles to infinity. After this, the similarity component ensures that the epipoles are on the X-axis, and guarantees the images are aligned. This is important as we only want to measure disparity along the baseline, so by ensuring the epipoles are on the X-axis we only need to search for horizontally aligned matches. Finally the shearing component ensures that the aspect ratio of our image is maintained after rectification. Without this, rectification might squash our image, reducing the number of useful pixels we have for the matching phase.</p>
<p>With our images now rectified and aligned, we can run a specialized feature matcher. This feature matcher takes advantage of the fact that matching points in our images should be horizontally aligned in order to do a more complete search with less false positive matches. This generates an estimated disparity value for each pixel in our image, which we can then use to calculate depth.</p>
<h1 id="design">Design</h1>
<p>Now that we have an understanding of the tools that are at our disposal to solve the problem, we can establish an initial design.</p>
<ol>
<li>Find matches between images.
<ul>
<li>Detect features and generate feature descriptions.</li>
<li>Match features by comparing their feature vectors.</li>
</ul></li>
<li>Estimate the fundamental matrix.
<ul>
<li>Use the 8-point algorithm to find the best F with the smallest median error</li>
</ul></li>
<li>Rectify and align our input images.
<ul>
<li>Use our matches and the fundamental matrix to generate a rectification homography</li>
<li>Adjust the shearing component of our homography to preserve aspect ratio</li>
<li>Apply the homography to our images</li>
</ul></li>
<li>Use a stereo matcher to find better matches, and estimate disparity for each pixel
<ul>
<li>OpenCV provides a special matcher for this use-case that uses a rolling window to find disparity for each pixel.</li>
</ul></li>
<li>Un-rectify our image
<ul>
<li>Apply the inverse of our rectification homography from the previous step to map our disparity back to our original image space</li>
</ul></li>
<li>Use focal length estimate to estimate depth for each pixel
<ul>
<li>We need to guess the baseline here</li>
</ul></li>
<li>Adjust the baseline guess based on user input
<ul>
<li>The user will input a vector and it's expected length, and we should be able to adjust our baseline estimate to match this.</li>
</ul></li>
</ol>
<p>This leaves a few areas of significance to research.</p>
